# ModelClash: Dynamic LLM Evaluation Arena

## 🚀 About ModelClash

ModelClash is a groundbreaking framework designed to evaluate and compare Large Language Models (LLMs) in a dynamic, competitive environment. By pitting AI models against each other in a series of creative problem-solving challenges, this project offers a novel approach to benchmarking that adapts to the rapid advancements in AI technology.

### 🌟 Key Features

- **Self-Generating Challenges**: Models create and solve complex problems, eliminating the need for manual dataset creation.
- **Adaptive Difficulty**: Automatically scales to match any level of AI intelligence.
- **Comprehensive Evaluation**: Tests a broad spectrum of skills including creativity, strategy, and self-awareness.

## 🧠 Why ModelClash?

Traditional benchmarks for Language Models often suffer from quick saturation and lack of adaptability. ModelClash addresses these issues with three core advantages:

1. **Automated, Testable Challenge Generation**
   - Eliminates the need for expert-created datasets
   - Reduces human bias in evaluation
   - Provides quantifiable results through an innovative scoring system

2. **Intelligence-Adaptive Testing**
   - Automatically adjusts to any level of model capability
   - Ensures long-term relevance as AI technology advances
   - Addresses the issue of benchmark saturation

3. **Holistic Intelligence Assessment**
   - Evaluates a wide range of cognitive skills
   - Tests both problem-solving and problem-creation abilities
   - Challenges models in creativity, strategy, and understanding of limitations

## 🛠 Getting Started

### Prerequisites

- Python 3.8+
- OpenAI API key
- Google API key (optional)
- Anthropic API key (optional)

### Installation

1. Clone the repo
   ```
   git clone https://github.com/mrconter1/ModelClash.git
   ```
2. Install required packages
   ```
   pip install -r requirements.txt
   ```
3. Set up your environment variables
   ```
   export OPENAI_API_KEY='your-api-key-here'
   export GOOGLE_API_KEY='your-api-key-here'
   export ANTHROPIC_API_KEY='your-api-key-here'
   ```

### Usage

Run the main script to start a competition:

```
python main.py
```

## 📊 Sample Output

```
Round 1
Model A challenge:
  Model A: +3, Model B: 0
Model B challenge:
  Model A: +2, Model B: +1

Round 2
...

Final Scores:
Model A: 152
Model B: 148

Winner: Model A
```

## 📈 Results

This section will be updated with the latest results from ModelClash competitions. Stay tuned for insights into the performance of various LLMs across different challenges.

## 🤝 Contributing

Contributions are what make the open-source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## 📜 License

Distributed under the MIT License. See `LICENSE` for more information.

## 📬 Contact

Rasmus Lindahl - rasmus.lindahl1996@gmail.com

Project Link: [https://github.com/mrconter1/ModelClash](https://github.com/mrconter1/ModelClash)

## 🙏 Acknowledgments

- [OpenAI](https://openai.com/)
- [Google AI](https://ai.google/)
- [Anthropic](https://www.anthropic.com/)

---

<p align="center">Made with ❤️ for the advancement of AI</p>